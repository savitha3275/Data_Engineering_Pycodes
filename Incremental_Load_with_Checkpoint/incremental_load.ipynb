{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a164de67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source orders created:\n",
      "   order_id  order_date customer   product  amount\n",
      "0         1  2024-06-01    Alice  Widget A    75.0\n",
      "1         2  2024-06-01      Bob  Widget B    50.0\n",
      "2         3  2024-06-02  Charlie  Widget A   150.0\n",
      "3         4  2024-06-02    Diana  Widget C    30.0\n",
      "4         5  2024-06-03    Alice  Widget B   100.0\n",
      "5         6  2024-06-03      Bob  Widget A    75.0\n",
      "6         7  2024-06-04      Eve  Widget C    60.0\n",
      "7         8  2024-06-04  Charlie  Widget B    50.0\n",
      "8         9  2024-06-05    Diana  Widget A   150.0\n",
      "9        10  2024-06-05    Alice  Widget C    90.0\n",
      "\n",
      "Total rows: 10\n",
      "Order ID range: 1 to 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create source data (simulating a production database table)\n",
    "data = {\n",
    "    \"order_id\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"order_date\": [\n",
    "        \"2024-06-01\", \"2024-06-01\", \"2024-06-02\", \"2024-06-02\", \"2024-06-03\",\n",
    "        \"2024-06-03\", \"2024-06-04\", \"2024-06-04\", \"2024-06-05\", \"2024-06-05\"\n",
    "    ],\n",
    "    \"customer\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Alice\",\n",
    "                 \"Bob\", \"Eve\", \"Charlie\", \"Diana\", \"Alice\"],\n",
    "    \"product\": [\"Widget A\", \"Widget B\", \"Widget A\", \"Widget C\", \"Widget B\",\n",
    "                \"Widget A\", \"Widget C\", \"Widget B\", \"Widget A\", \"Widget C\"],\n",
    "    \"amount\": [75.00, 50.00, 150.00, 30.00, 100.00, \n",
    "               75.00, 60.00, 50.00, 150.00, 90.00]\n",
    "}\n",
    "\n",
    "source_df = pd.DataFrame(data)\n",
    "source_df.to_csv(\"source_orders.csv\", index=False)\n",
    "\n",
    "print(\"Source orders created:\")\n",
    "print(source_df)\n",
    "print(f\"\\nTotal rows: {len(source_df)}\")\n",
    "print(f\"Order ID range: {source_df['order_id'].min()} to {source_df['order_id'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1adcf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty warehouse file created.\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(columns=source_df.columns).to_csv(\"warehouse_orders.csv\", index=False)\n",
    "print(\"Empty warehouse file created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83beee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FILE = \"checkpoint.json\"\n",
    "\n",
    "def read_checkpoint():\n",
    "    \"\"\"\n",
    "    Read the last processed order_id from the checkpoint file.\n",
    "    Returns 0 if no checkpoint exists (first run).\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "        print(f\"Checkpoint found: last_processed_id = {checkpoint['last_processed_id']}\")\n",
    "        return checkpoint[\"last_processed_id\"]\n",
    "    else:\n",
    "        print(\"No checkpoint found — this is the FIRST RUN\")\n",
    "        return 0  # No checkpoint = start from beginning\n",
    "\n",
    "def write_checkpoint(last_processed_id):\n",
    "    \"\"\"\n",
    "    Save the last processed order_id to the checkpoint file.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        \"last_processed_id\": last_processed_id,\n",
    "        \"updated_at\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "    print(f\"Checkpoint updated: last_processed_id = {last_processed_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0baf85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found — this is the FIRST RUN\n",
      "Starting from order_id > 0\n",
      "Checkpoint updated: last_processed_id = 5\n",
      "Checkpoint found: last_processed_id = 5\n",
      "After writing 5, checkpoint returns: 5\n",
      "Checkpoint removed for fresh start\n"
     ]
    }
   ],
   "source": [
    "last_id = read_checkpoint()\n",
    "print(f\"Starting from order_id > {last_id}\")\n",
    "\n",
    "write_checkpoint(5)\n",
    "last_id = read_checkpoint()\n",
    "print(f\"After writing 5, checkpoint returns: {last_id}\")\n",
    "\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    os.remove(CHECKPOINT_FILE)\n",
    "    print(\"Checkpoint removed for fresh start\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a14baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental Load Function\n",
    "def incremental_load(source_file, warehouse_file):\n",
    "    \"\"\"\n",
    "    Load only NEW rows from source since the last checkpoint.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 55}\")\n",
    "    print(f\"  INCREMENTAL LOAD\")\n",
    "    print(f\"{'=' * 55}\")\n",
    "    \n",
    "    # Step 1: Read checkpoint\n",
    "    last_processed_id = read_checkpoint()\n",
    "    \n",
    "    # Step 2: Read source and filter new rows\n",
    "    source_df = pd.read_csv(source_file)\n",
    "    new_rows = source_df[source_df[\"order_id\"] > last_processed_id]\n",
    "    \n",
    "    print(f\"\\nSource total rows: {len(source_df)}\")\n",
    "    print(f\"New rows (order_id > {last_processed_id}): {len(new_rows)}\")\n",
    "    \n",
    "    # Step 3: Handle no new data\n",
    "    if len(new_rows) == 0:\n",
    "        print(\"No new data to load. Skipping.\")\n",
    "        return pd.read_csv(warehouse_file) if os.path.exists(warehouse_file) else pd.DataFrame()\n",
    "    \n",
    "    # Step 4: Append to warehouse\n",
    "    if os.path.exists(warehouse_file) and os.path.getsize(warehouse_file) > 0:\n",
    "        warehouse_df = pd.read_csv(warehouse_file)\n",
    "    else:\n",
    "        warehouse_df = pd.DataFrame(columns=source_df.columns)\n",
    "    \n",
    "    print(f\"Warehouse rows BEFORE: {len(warehouse_df)}\")\n",
    "    \n",
    "    warehouse_df = pd.concat([warehouse_df, new_rows], ignore_index=True)\n",
    "    warehouse_df.to_csv(warehouse_file, index=False)\n",
    "    \n",
    "    print(f\"Warehouse rows AFTER: {len(warehouse_df)}\")\n",
    "    \n",
    "    # Step 5: Update checkpoint\n",
    "    new_max_id = int(new_rows[\"order_id\"].max())\n",
    "    write_checkpoint(new_max_id)\n",
    "    \n",
    "    print(f\"\\n--- Incremental Load Summary ---\")\n",
    "    print(f\"New rows loaded: {len(new_rows)}\")\n",
    "    print(f\"New checkpoint: {new_max_id}\")\n",
    "    \n",
    "    return warehouse_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5788af8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean state: no checkpoint, empty warehouse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    os.remove(CHECKPOINT_FILE)\n",
    "\n",
    "pd.DataFrame(columns=[\"order_id\",\"order_date\",\"customer\",\"product\",\"amount\"]).to_csv(\"warehouse_orders.csv\", index=False)\n",
    "\n",
    "print(\"Clean state: no checkpoint, empty warehouse\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e046cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== FIRST RUN (no checkpoint) =====\n",
      "\n",
      "=======================================================\n",
      "  INCREMENTAL LOAD\n",
      "=======================================================\n",
      "No checkpoint found — this is the FIRST RUN\n",
      "\n",
      "Source total rows: 10\n",
      "New rows (order_id > 0): 10\n",
      "Warehouse rows BEFORE: 0\n",
      "Warehouse rows AFTER: 10\n",
      "Checkpoint updated: last_processed_id = 10\n",
      "\n",
      "--- Incremental Load Summary ---\n",
      "New rows loaded: 10\n",
      "New checkpoint: 10\n",
      "\n",
      "Warehouse contents:\n",
      "  order_id  order_date customer   product  amount\n",
      "0        1  2024-06-01    Alice  Widget A    75.0\n",
      "1        2  2024-06-01      Bob  Widget B    50.0\n",
      "2        3  2024-06-02  Charlie  Widget A   150.0\n",
      "3        4  2024-06-02    Diana  Widget C    30.0\n",
      "4        5  2024-06-03    Alice  Widget B   100.0\n",
      "5        6  2024-06-03      Bob  Widget A    75.0\n",
      "6        7  2024-06-04      Eve  Widget C    60.0\n",
      "7        8  2024-06-04  Charlie  Widget B    50.0\n",
      "8        9  2024-06-05    Diana  Widget A   150.0\n",
      "9       10  2024-06-05    Alice  Widget C    90.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Savithadevi\\AppData\\Local\\Temp\\ipykernel_53920\\2189515007.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  warehouse_df = pd.concat([warehouse_df, new_rows], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"===== FIRST RUN (no checkpoint) =====\")\n",
    "result = incremental_load(\"source_orders.csv\", \"warehouse_orders.csv\")\n",
    "print(\"\\nWarehouse contents:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0edfb72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source now has 13 rows\n"
     ]
    }
   ],
   "source": [
    "# Simulate New Data\n",
    "new_orders = pd.DataFrame({\n",
    "    \"order_id\": [11, 12, 13],\n",
    "    \"order_date\": [\"2024-06-06\", \"2024-06-06\", \"2024-06-07\"],\n",
    "    \"customer\": [\"Frank\", \"Alice\", \"Bob\"],\n",
    "    \"product\": [\"Widget A\", \"Widget B\", \"Widget C\"],\n",
    "    \"amount\": [200.00, 75.00, 45.00]\n",
    "})\n",
    "\n",
    "source_df = pd.read_csv(\"source_orders.csv\")\n",
    "source_df = pd.concat([source_df, new_orders], ignore_index=True)\n",
    "source_df.to_csv(\"source_orders.csv\", index=False)\n",
    "\n",
    "print(f\"Source now has {len(source_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91151ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SECOND RUN =====\n",
      "\n",
      "=======================================================\n",
      "  INCREMENTAL LOAD\n",
      "=======================================================\n",
      "Checkpoint found: last_processed_id = 10\n",
      "\n",
      "Source total rows: 13\n",
      "New rows (order_id > 10): 3\n",
      "Warehouse rows BEFORE: 10\n",
      "Warehouse rows AFTER: 13\n",
      "Checkpoint updated: last_processed_id = 13\n",
      "\n",
      "--- Incremental Load Summary ---\n",
      "New rows loaded: 3\n",
      "New checkpoint: 13\n",
      "\n",
      "Total warehouse rows: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== SECOND RUN =====\")\n",
    "result = incremental_load(\"source_orders.csv\", \"warehouse_orders.csv\")\n",
    "print(\"\\nTotal warehouse rows:\", len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94af8dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== THIRD RUN (no new data) =====\n",
      "\n",
      "=======================================================\n",
      "  INCREMENTAL LOAD\n",
      "=======================================================\n",
      "Checkpoint found: last_processed_id = 13\n",
      "\n",
      "Source total rows: 13\n",
      "New rows (order_id > 13): 0\n",
      "No new data to load. Skipping.\n",
      "\n",
      "Total warehouse rows: 13\n"
     ]
    }
   ],
   "source": [
    "# No New Data\n",
    "\n",
    "print(\"\\n===== THIRD RUN (no new data) =====\")\n",
    "result = incremental_load(\"source_orders.csv\", \"warehouse_orders.csv\")\n",
    "print(\"\\nTotal warehouse rows:\", len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7b0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf113c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
