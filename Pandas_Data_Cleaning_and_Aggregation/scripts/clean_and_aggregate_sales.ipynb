{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cde08cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.read_csv(\"../data/sales_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0bfe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET INSPECTION ===\n",
      "Shape: (6, 5)\n",
      "\n",
      "Data types:\n",
      "order_id      int64\n",
      "region       object\n",
      "product      object\n",
      "quantity    float64\n",
      "price         int64\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "order_id    0\n",
      "region      0\n",
      "product     0\n",
      "quantity    1\n",
      "price       0\n",
      "dtype: int64\n",
      "\n",
      "First few rows:\n",
      "   order_id region   product  quantity  price\n",
      "0      4001   East  Keyboard       2.0   1500\n",
      "1      4002   West     Mouse       NaN    500\n",
      "2      4003   East   Monitor       1.0  12000\n",
      "3      4004  South  Keyboard       1.0   1500\n",
      "4      4005   West   Monitor       2.0  12000\n",
      "\n",
      "Duplicate rows: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DATASET INSPECTION ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nFirst few rows:\\n{df.head()}\")\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de74f397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before removing duplicates: 6\n",
      "Rows after removing duplicates: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows before removing duplicates: {len(df)}\")\n",
    "df_clean = df.drop_duplicates()\n",
    "print(f\"Rows after removing duplicates: {len(df_clean)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e71584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows before dropping missing quantity: 5\n",
      "Rows after dropping missing quantity: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\nRows before dropping missing quantity: {len(df_clean)}\")\n",
    "df_clean = df_clean.dropna(subset=[\"quantity\"])\n",
    "print(f\"Rows after dropping missing quantity: {len(df_clean)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3380f2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA TYPES AFTER CLEANING ===\n",
      "order_id      int64\n",
      "region       object\n",
      "product      object\n",
      "quantity    float64\n",
      "price         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_clean[\"quantity\"] = pd.to_numeric(df_clean[\"quantity\"], errors='coerce')\n",
    "df_clean[\"price\"] = pd.to_numeric(df_clean[\"price\"], errors='coerce')\n",
    "\n",
    "print(\"\\n=== DATA TYPES AFTER CLEANING ===\")\n",
    "print(df_clean.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb1529fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CLEANING VERIFICATION ===\n",
      "Missing values:\n",
      "order_id    0\n",
      "region      0\n",
      "product     0\n",
      "quantity    0\n",
      "price       0\n",
      "dtype: int64\n",
      "Duplicates: 0\n",
      "\n",
      "Cleaned dataset:\n",
      "   order_id region   product  quantity  price\n",
      "0      4001   East  Keyboard       2.0   1500\n",
      "2      4003   East   Monitor       1.0  12000\n",
      "3      4004  South  Keyboard       1.0   1500\n",
      "4      4005   West   Monitor       2.0  12000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CLEANING VERIFICATION ===\")\n",
    "print(f\"Missing values:\\n{df_clean.isnull().sum()}\")\n",
    "print(f\"Duplicates: {df_clean.duplicated().sum()}\")\n",
    "print(f\"\\nCleaned dataset:\\n{df_clean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26dcab18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with revenue:\n",
      "   order_id region   product  quantity  price  revenue\n",
      "0      4001   East  Keyboard       2.0   1500   3000.0\n",
      "2      4003   East   Monitor       1.0  12000  12000.0\n",
      "3      4004  South  Keyboard       1.0   1500   1500.0\n",
      "4      4005   West   Monitor       2.0  12000  24000.0\n"
     ]
    }
   ],
   "source": [
    "df_clean[\"revenue\"] = df_clean[\"quantity\"] * df_clean[\"price\"]\n",
    "print(\"Dataset with revenue:\")\n",
    "print(df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5aea5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REVENUE BY PRODUCT ===\n",
      "    product  total_revenue\n",
      "1   Monitor        36000.0\n",
      "0  Keyboard         4500.0\n"
     ]
    }
   ],
   "source": [
    "revenue_by_product = df_clean.groupby(\"product\")[\"revenue\"].sum().reset_index()\n",
    "revenue_by_product.columns = [\"product\", \"total_revenue\"]\n",
    "revenue_by_product = revenue_by_product.sort_values(\"total_revenue\", ascending=False)\n",
    "\n",
    "print(\"\\n=== REVENUE BY PRODUCT ===\")\n",
    "print(revenue_by_product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fcf3862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REVENUE BY REGION ===\n",
      "  region  total_revenue\n",
      "2   West        24000.0\n",
      "0   East        15000.0\n",
      "1  South         1500.0\n"
     ]
    }
   ],
   "source": [
    "revenue_by_region = df_clean.groupby(\"region\")[\"revenue\"].sum().reset_index()\n",
    "revenue_by_region.columns = [\"region\", \"total_revenue\"]\n",
    "revenue_by_region = revenue_by_region.sort_values(\"total_revenue\", ascending=False)\n",
    "\n",
    "print(\"\\n=== REVENUE BY REGION ===\")\n",
    "print(revenue_by_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37834068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: clean_sales.csv\n"
     ]
    }
   ],
   "source": [
    "df_clean.to_csv(\"clean_sales.csv\", index=False)\n",
    "print(\"Saved: clean_sales.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b9405aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: revenue_by_product.csv\n"
     ]
    }
   ],
   "source": [
    "revenue_by_product.to_csv(\"revenue_by_product.csv\", index=False)\n",
    "print(\"Saved: revenue_by_product.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d91c235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: revenue_by_region.csv\n"
     ]
    }
   ],
   "source": [
    "revenue_by_region.to_csv(\"revenue_by_region.csv\", index=False)\n",
    "print(\"Saved: revenue_by_region.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63e7e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY ===\n",
      "Total records cleaned: 4\n",
      "Total revenue: 40,500.00\n",
      "Products analyzed: 2\n",
      "Regions analyzed: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Total records cleaned: {len(df_clean)}\")\n",
    "print(f\"Total revenue: {df_clean['revenue'].sum():,.2f}\")\n",
    "print(f\"Products analyzed: {len(revenue_by_product)}\")\n",
    "print(f\"Regions analyzed: {len(revenue_by_region)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd75bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_aggregate_sales(input_file):\n",
    "    \"\"\"\n",
    "    Clean sales data and generate revenue summaries.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Loaded {len(df)} records\")\n",
    "    \n",
    "    # Clean data\n",
    "    df_clean = df.drop_duplicates()\n",
    "    df_clean = df_clean.dropna(subset=[\"quantity\"])\n",
    "    df_clean[\"quantity\"] = pd.to_numeric(df_clean[\"quantity\"], errors='coerce')\n",
    "    df_clean[\"price\"] = pd.to_numeric(df_clean[\"price\"], errors='coerce')\n",
    "    \n",
    "    # Calculate revenue\n",
    "    df_clean[\"revenue\"] = df_clean[\"quantity\"] * df_clean[\"price\"]\n",
    "    \n",
    "    # Aggregations\n",
    "    revenue_by_product = df_clean.groupby(\"product\")[\"revenue\"].sum().reset_index()\n",
    "    revenue_by_product.columns = [\"product\", \"total_revenue\"]\n",
    "    revenue_by_product = revenue_by_product.sort_values(\"total_revenue\", ascending=False)\n",
    "    \n",
    "    revenue_by_region = df_clean.groupby(\"region\")[\"revenue\"].sum().reset_index()\n",
    "    revenue_by_region.columns = [\"region\", \"total_revenue\"]\n",
    "    revenue_by_region = revenue_by_region.sort_values(\"total_revenue\", ascending=False)\n",
    "    \n",
    "    # Save outputs\n",
    "    df_clean.to_csv(\"../output/clean_sales.csv\", index=False)\n",
    "    revenue_by_product.to_csv(\"../output/revenue_by_product.csv\", index=False)\n",
    "    revenue_by_region.to_csv(\"../output/revenue_by_region.csv\", index=False)\n",
    "    \n",
    "    print(\"Cleaning and aggregation complete!\")\n",
    "    print(f\"  - Clean records: {len(df_clean)}\")\n",
    "    print(f\"  - Total revenue: {df_clean['revenue'].sum():,.2f}\")\n",
    "    \n",
    "    return df_clean, revenue_by_product, revenue_by_region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50fd0189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 records\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clean_df, product_rev, region_rev \u001b[38;5;241m=\u001b[39m clean_and_aggregate_sales(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/sales_raw.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 28\u001b[0m, in \u001b[0;36mclean_and_aggregate_sales\u001b[1;34m(input_file)\u001b[0m\n\u001b[0;32m     25\u001b[0m revenue_by_region \u001b[38;5;241m=\u001b[39m revenue_by_region\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_revenue\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Save outputs\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m df_clean\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../output/clean_sales.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m revenue_by_product\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../output/revenue_by_product.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m revenue_by_region\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../output/revenue_by_region.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\generic.py:3986\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3975\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3977\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3978\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3979\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3983\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3984\u001b[0m )\n\u001b[1;32m-> 3986\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3987\u001b[0m     path_or_buf,\n\u001b[0;32m   3988\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3989\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3990\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3991\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3992\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3993\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3994\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3995\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3996\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3997\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3998\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3999\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   4000\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   4001\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   4002\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   4003\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     check_parent_directory(\u001b[38;5;28mstr\u001b[39m(handle))\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '..\\output'"
     ]
    }
   ],
   "source": [
    "clean_df, product_rev, region_rev = clean_and_aggregate_sales(\"../data/sales_raw.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bdf022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
